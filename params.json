{"body":"     _                         __  __  ____ __  __  ____ \r\n    | |    __ _ _ __ ___   ___|  \\/  |/ ___|  \\/  |/ ___|\r\n    | |   / _` | '_ ` _ \\ / _ \\ |\\/| | |   | |\\/| | |    \r\n    | |__| (_| | | | | | |  __/ |  | | |___| |  | | |___ \r\n    |_____\\__,_|_| |_| |_|\\___|_|  |_|\\____|_|  |_|\\____|\r\n                                      The Lua markov bot!\r\n\r\nCourtesy of @enkiv2\r\n\r\n## LameMCMC is different\r\n* The markov model is second-order, not first order (like most bots).\r\n* Tokenization is performed by splitting spaces and punctuation away from alphanumeric characters. So, chunks of punctuation is preserved and words are not linked to their punctuation. This way, you can have just as much coherence as your whitespace-tokenizing first-order bot with less training data and a smaller database.\r\n* Three layers of ranking eliminate the worst of a pool of possible responses. If the bot doesn't have anything nice to say (where niceness is a configurable ranking level), it doesn't say it.\r\n\r\n## This distribution contains:\r\n\r\n* mm.lua -- the guts: implementation of training, response generation & ranking\r\n* dumper.lua -- serializes lua tables\r\n* fe.lua -- frontend: everything with file i/o or irc-specific code\r\n* ircmode.sh -- connects to irc using epic/ircii\r\n* mm.db -- the serialized representation of the current training data\r\n\r\n## What do I need?\r\nLameMCMC has a handful of requirements:\r\n* A unix-like OS\r\n* bash\r\n* lua\r\n* a dictionary file in /usr/share/dict/words\r\n* (irc mode only) ircii or epic (or any irc client with a dumb-terminal mode that places angle brackets around nicks)\r\n* (optional) figlet","name":"Lamemcmc","tagline":"A markov chain bot in lua","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}